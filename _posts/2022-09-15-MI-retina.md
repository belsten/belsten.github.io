---
title: "Mutual information based retinal coding model derivation"
date: 2022-09-15
categories:
  - blog
tags:
  - minimal-mistakes
  - Jekyll
  - update
---

This post aims to derive the mutual information object for the retinal coding model originally introduced in 2011 by Yan Karklin and Eero Simoncelli in a paper titled *Efficient coding of natural images with a population of noisy Linear- Nonlinear neurons*. It is a fantastic exemplar of theoretical neuroscience:  biology guides the model's constraints, and theory guides its objective, ultimately resulting in the functional properties of the retina.

Subsequent papers have used the model originally introduced by Karkin and Simoncelli to explore other aspects of retinal coding. I list the following, as they have guided the form of the model and its derivation:
* *Inter-mosaic coordination of retinal receptive fields* Nature 2021 by Roy et al.
* *Scene statistics and noise determine the relative arrangement of receptive field mosaics* PNAS 2021 by Jun, Field, and Pearson.


Given my interest in retinal coding, I decided it would be worthwhile to implement the model myself since I had some ideas for extensions. However, when it came down to translating equations into code, I found myself in a pickle. And this pickle was a major exercise of error correction.

In this post, I'm going to attempt to derive the model and it's final object from first principles, which will serve as a reference for myself and, hopefully, anyone else who decides they're also going to implement a similar model.

---
